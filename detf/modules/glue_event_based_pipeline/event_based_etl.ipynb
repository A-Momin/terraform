{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "import os, time, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "from misc import load_from_yaml, save_to_yaml\n",
    "import s3, iam, lftn, glue, lambdafn as lfn, sns, eventbridge as event\n",
    "from lambdafn import build_lambda_package, print_latest_lambda_logs\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.getenv(\"MY_AWS_DIR\", \"\") + \"/.env\")\n",
    "\n",
    "from mylogger import CustomLogger\n",
    "logger = CustomLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_ID        = os.environ['AWS_ACCOUNT_ID_ROOT']\n",
    "REGION            = os.environ.get('AWS_DEFAULT_REGION', 'us-east-1')\n",
    "VPC_ID            = os.environ['AWS_DEFAULT_VPC']\n",
    "SECURITY_GROUP_ID = os.environ['AWS_DEFAULT_SG_ID']\n",
    "SUBNET_IDS        = SUBNET_IDS = os.environ[\"AWS_DEFAULT_SUBNET_IDS\"].split(\":\")\n",
    "SUBNET_ID         = SUBNET_IDS[0]\n",
    "\n",
    "logger.info(f\"VPC_ID: {ACCOUNT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_client           = boto3.client('sts')\n",
    "rds_client           = boto3.client('rds')\n",
    "iam_client           = boto3.client('iam')\n",
    "s3_client            = boto3.client('s3')\n",
    "glue_client          = boto3.client('glue')\n",
    "lakeformation_client = boto3.client('lakeformation')\n",
    "ec2_client           = boto3.client('ec2', region_name=REGION)\n",
    "ec2_resource         = boto3.resource('ec2', region_name=REGION)\n",
    "dynamodb_client      = boto3.client('dynamodb')\n",
    "events_client        = boto3.client('events')\n",
    "lambda_client        = boto3.client('lambda')\n",
    "sns_client           = boto3.client('sns')\n",
    "cw_logs_client       = boto3.client('logs')\n",
    "logs_client = boto3.client(\"logs\")\n",
    "\n",
    "# Create a CloudWatch client for Logs\n",
    "logs_client = boto3.client('logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Knowledge Amplifier: Build Serverless DataLake using Glue , Lambda , Cloudwatch](https://www.youtube.com/watch?v=3f7UY5R9Q9U&t=0s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\" ><b style=\"color:green\">WORKING AS EXPECTED</b></div>\n",
    "\n",
    "<div style=\"text-align:center\" ><img src=\"./design_diagram.png\" width=\"600\" height=\"300\" /></div>\n",
    "\n",
    "- Demonstration of adding external libraries to Glue Job\n",
    "- Demonstrate how Incremental Data Processing (Job Bookmark) works in Glue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Pickled Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"event_based_etl_ipynb.pkl\", \"rb\") as f:\n",
    "    GLUE_ROLE_NAME, LFN_ROLE_NAME, GLUE_ROLE_ARN, LFN_ROLE_ARN, S3_BUCKET_DATALAKE, S3_BUCKET_GLUE_ASSETS, GLUE_CATALOG_DB, DATALAKE_LOCATION_URI, TOPIC_NAME, JOB_COMPLETE_TOPIC_ARN, LFN_CRAWLER_NAME, LFN_CRAWLER_ARN, S3_CUSTOMER_CRAWLER_NAME, S3_CUSTOMER_CRAWLER_TARGET, S3_SALES_CRAWLER, S3_SALES_CRAWLER_TARGET, LFN_JOB_TRIGGERER_NAME, LFN_JOB_TRIGGERER_ARN, TEM_DIR, SPARK_EVENT_LOG_PATH, TABLE_NAME, TARGET, JOB_NAME, JOB_SCRIPT_LOCATION, ARGS, S3_CUSTOMER_CRAWLER_RULE_NAME, S3_CUSTOMER_CRAWLER_RULE_ARN, JOB_COMPLETE_RULE_NAME, JOB_COMPLETE_RULE_ARN = pickle.load(f)\n",
    "\n",
    "print(GLUE_ROLE_NAME, LFN_ROLE_NAME, GLUE_ROLE_ARN, LFN_ROLE_ARN, S3_BUCKET_DATALAKE, S3_BUCKET_GLUE_ASSETS, GLUE_CATALOG_DB, DATALAKE_LOCATION_URI, TOPIC_NAME, JOB_COMPLETE_TOPIC_ARN, LFN_CRAWLER_NAME, LFN_CRAWLER_ARN, S3_CUSTOMER_CRAWLER_NAME, S3_CUSTOMER_CRAWLER_TARGET, S3_SALES_CRAWLER, S3_SALES_CRAWLER_TARGET, LFN_JOB_TRIGGERER_NAME, LFN_JOB_TRIGGERER_ARN, TEM_DIR, SPARK_EVENT_LOG_PATH, TABLE_NAME, TARGET, JOB_NAME, JOB_SCRIPT_LOCATION, ARGS, S3_CUSTOMER_CRAWLER_RULE_NAME, S3_CUSTOMER_CRAWLER_RULE_ARN, JOB_COMPLETE_RULE_NAME, JOB_COMPLETE_RULE_ARN, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create aws glue role by the name of `glue_role_name`.\n",
    "- Assign Power User Access Policy (`PowerUserAccess`) to the role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_ROLE_NAME = 'glue-pipeline-role'\n",
    "LFN_ROLE_NAME = 'lfn-pipeline-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_arns = [\n",
    "    \"arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole\",\n",
    "    \"arn:aws:iam::aws:policy/CloudWatchFullAccess\",\n",
    "    \"arn:aws:iam::aws:policy/AmazonS3FullAccess\",\n",
    "    \"arn:aws:iam::aws:policy/AdministratorAccess\",\n",
    "    \"arn:aws:iam::aws:policy/PowerUserAccess\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Glue Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_doc = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"glue.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "GLUE_ROLE_ARN = iam_client.create_role(\n",
    "    RoleName=GLUE_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_doc),\n",
    "    Description=\"Glue Service Role\"\n",
    ")['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach AWS managed policy with the role\n",
    "[iam_client.attach_role_policy(RoleName=GLUE_ROLE_NAME, PolicyArn=parn) for parn in policy_arns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue_put_event_policy = {\n",
    "#   \"Version\": \"2012-10-17\",\n",
    "#   \"Statement\": [\n",
    "#     {\n",
    "#       \"Effect\": \"Allow\",\n",
    "#       \"Action\": [\n",
    "#         \"events:PutEvents\"\n",
    "#       ],\n",
    "#       \"Resource\": \"*\"\n",
    "#     }\n",
    "#   ]\n",
    "# }\n",
    "\n",
    "# # Attach the inline policy to the IAM role\n",
    "# iam_client.put_role_policy(\n",
    "#     RoleName=GLUE_ROLE_NAME,\n",
    "#     PolicyName=\"glue_put_event\",\n",
    "#     PolicyDocument=json.dumps(glue_put_event_policy)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lambda Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"lambda.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create the IAM role with the assume role policy document\n",
    "LFN_ROLE_ARN = iam_client.create_role(\n",
    "    RoleName=LFN_ROLE_NAME,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    ")['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[iam_client.attach_role_policy(RoleName=LFN_ROLE_NAME, PolicyArn=parn) for parn in policy_arns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #### Create IAM Role Policy (, S3, Logs Permissions)\n",
    "# policy_document = {\n",
    "#     \"Version\": \"2012-10-17\",\n",
    "#     \"Statement\": [\n",
    "#         {   # StartCrawler permission\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": [\n",
    "#                 \"glue:StartCrawler\"\n",
    "#             ],\n",
    "#             \"Resource\": f\"arn:aws:glue:region:account-id:crawler/*\"\n",
    "#             # \"Resource\": f\"arn:aws:glue:region:account-id:crawler/{crawler-name}\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": [\n",
    "#                 \"glue:StartJobRun\",\n",
    "#                 \"glue:GetJob\",\n",
    "#                 \"glue:GetJobRun\"\n",
    "#             ],\n",
    "#             \"Resource\": f\"arn:aws:glue:region:account-id:job/*\"\n",
    "#         },\n",
    "#         {   # s3 full access\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": [\n",
    "#                 \"s3:*\",\n",
    "#                 \"s3-object-lambda:*\"\n",
    "#             ],\n",
    "#             \"Resource\": \"*\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"Effect\": \"Allow\",\n",
    "#             \"Action\": [\n",
    "#                 \"logs:*\"\n",
    "#             ],\n",
    "#             \"Resource\": \"*\"\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# policy_name = \"s3_logs_policies\"\n",
    "\n",
    "# # Attach the inline policy to the IAM role\n",
    "# iam_client.put_role_policy(\n",
    "#     RoleName=LFN_ROLE_NAME,\n",
    "#     PolicyName=policy_name,\n",
    "#     PolicyDocument=json.dumps(policy_document)\n",
    "# )\n",
    "# print(f\"Policy {policy_name} attached to role {LFN_ROLE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(LFN_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create S3 Bucket and Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET_DATALAKE = \"htech-datalake-bkt\"\n",
    "S3_BUCKET_GLUE_ASSETS = \"htech-glue-assets-bkt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl = 'public-read'                         # Set the ACL (e.g., 'private', 'public-read')\n",
    "enable_versioning = False                   # Enable versioning\n",
    "enable_encryption = False                   # Enable server-side encryption\n",
    "\n",
    "prefixes1 = [\n",
    "    \"raw/customers/\",\n",
    "    \"raw/sales/\",\n",
    "    \"processed/customers/\",\n",
    "    \"processed/sales/\",\n",
    "    \"misc/\"\n",
    "]\n",
    "prefixes2 = [\n",
    "    \"temporary/\",\n",
    "    \"sparkHistoryLogs/\",\n",
    "    \"libraries/\",\n",
    "    \"misc/\"\n",
    "]\n",
    "\n",
    "s3.create_s3_bucket(S3_BUCKET_DATALAKE, prefixes1)\n",
    "s3.create_s3_bucket(S3_BUCKET_GLUE_ASSETS, prefixes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Glue Catalog Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_CATALOG_DB = 'htech-glue-catalog-db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example usage\n",
    "DATALAKE_LOCATION_URI = f\"s3://{S3_BUCKET_DATALAKE}\"\n",
    "\n",
    "create_database_response = glue_client.create_database(\n",
    "    CatalogId=ACCOUNT_ID,\n",
    "    DatabaseInput={\n",
    "        \"Name\": GLUE_CATALOG_DB,\n",
    "        \"Description\": \"\",\n",
    "        \"LocationUri\": DATALAKE_LOCATION_URI,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(create_database_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = glue_client.get_database(CatalogId=ACCOUNT_ID, Name=CATALOG_DB_NAME)\n",
    "# print(json.dumps(response, indent=4, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grant `CREATE_TABLE` permission to `glue_role_name` on data catalog DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arn for glue_role_name\n",
    "lf_principle = GLUE_ROLE_ARN\n",
    "\n",
    "# Grant 'CREATE_TABLE' and ''DROP LF Permission to `glue_role_name` Role\n",
    "response = lakeformation_client.grant_permissions(\n",
    "    Principal={\"DataLakePrincipalIdentifier\": lf_principle},\n",
    "    Resource={\"Database\": {\"Name\": GLUE_CATALOG_DB}},\n",
    "    Permissions=[\"CREATE_TABLE\", \"DROP\"],\n",
    "    PermissionsWithGrantOption=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lftn.grant_table_level_permissions(GLUE_ROLE_ARN, CATALOG_DB_NAME, 'employees', ['DROP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_NAME = \"htech-sns-topic\"\n",
    "JOB_COMPLETE_TOPIC_ARN = \"arn:aws:sns:us-east-1:530976901147:htech-sns-topic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_COMPLETE_TOPIC_ARN = sns_client.create_topic(Name=TOPIC_NAME)['TopicArn']\n",
    "\n",
    "protocol=\"email\"\n",
    "endpoint=\"bbcredcap3@gmail.com\"\n",
    "\n",
    "sns_client.subscribe(\n",
    "    TopicArn=JOB_COMPLETE_TOPIC_ARN,\n",
    "    Protocol=protocol,\n",
    "    Endpoint=endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attributes\n",
    "response = sns_client.get_topic_attributes(TopicArn=JOB_COMPLETE_TOPIC_ARN)\n",
    "\n",
    "# # Print attributes\n",
    "# for key, value in response[\"Attributes\"].items():\n",
    "#     logger.info(f\"{key}: {value}\")\n",
    "\n",
    "logger.info(response[\"Attributes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Lambda 1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfn_scripts = [\"lambdas/lfn1/crawler_triggerer.py\"]\n",
    "build_lambda_package(lfn_scripts, \"lambdas/lfn1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFN_CRAWLER_NAME = \"crawler_triggerer\"\n",
    "zip_file = \"./lambdas/lfn1/package.zip\"  # Change this to the actual zip file path\n",
    "\n",
    "# Create Lambda function\n",
    "with open(zip_file, 'rb') as f:\n",
    "    zipped_code = f.read()\n",
    "\n",
    "LFN_CRAWLER_ARN = lambda_client.create_function(\n",
    "    FunctionName=LFN_CRAWLER_NAME,\n",
    "    Runtime='python3.9',\n",
    "    Role=LFN_ROLE_ARN,\n",
    "    Handler='crawler_triggerer.lambda_handler',\n",
    "    Code={'ZipFile': zipped_code},\n",
    "    Timeout=120,\n",
    "    Environment={\n",
    "        'Variables': {\n",
    "            'foo': 'BAR'\n",
    "        }\n",
    "    }\n",
    ")['FunctionArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lfn.update_lambda_function_code(LFN_CRAWLER_NAME, zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = lambda_client.add_permission(\n",
    "    FunctionName=LFN_CRAWLER_NAME,  # Replace with your Lambda function name\n",
    "    StatementId='s3-invoke-permission',  # An identifier for this statement, unique for each permission you add\n",
    "    Action='lambda:InvokeFunction',\n",
    "    Principal='s3.amazonaws.com',\n",
    "    SourceArn=f\"arn:aws:s3:::{S3_BUCKET_DATALAKE}\",\n",
    "    SourceAccount=ACCOUNT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add S3 trigger to the Lambda function\n",
    "response = s3_client.put_bucket_notification_configuration(\n",
    "    Bucket=S3_BUCKET_DATALAKE,\n",
    "    NotificationConfiguration={\n",
    "        'LambdaFunctionConfigurations': [\n",
    "            {\n",
    "                'LambdaFunctionArn': LFN_CRAWLER_ARN,\n",
    "                'Events': [\n",
    "                    's3:ObjectCreated:*'  # Trigger Lambda on object creation\n",
    "                ],\n",
    "                'Filter': {\n",
    "                    'Key': {\n",
    "                        'FilterRules': [\n",
    "                            {\n",
    "                                'Name': 'prefix',\n",
    "                                'Value': 'raw/customers/'  # Trigger only on this prefix\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "logger.info(\"S3 bucket notification configuration updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Amazon S3 to trigger a Lambda function, you cannot directly attach a custom payload to the Lambda event â€” S3 always sends a predefined event structure that includes information about the object created, such as:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Records\": [\n",
    "    {\n",
    "      \"eventSource\": \"aws:s3\",\n",
    "      \"eventName\": \"ObjectCreated:Put\",\n",
    "      \"s3\": {\n",
    "        \"bucket\": {\n",
    "          \"name\": \"your-bucket\",\n",
    "        },\n",
    "        \"object\": {\n",
    "          \"key\": \"some/path/file.csv\",\n",
    "          \"size\": 12345\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Crawler 1**: Catalog Data from `raw/customers` as a table by the name `raw_customers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_CUSTOMER_CRAWLER_NAME = \"htech-s3-customer-crawler\"\n",
    "S3_CUSTOMER_CRAWLER_TARGET = {\n",
    "    'S3Targets': [{'Path': f\"s3://{S3_BUCKET_DATALAKE}/{'raw/customers'}\"},]\n",
    "}\n",
    "glue.create_glue_crawler(\n",
    "    S3_CUSTOMER_CRAWLER_NAME,\n",
    "    GLUE_ROLE_ARN,\n",
    "    GLUE_CATALOG_DB,\n",
    "    S3_CUSTOMER_CRAWLER_TARGET,\n",
    "    table_prefix=\"raw_\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue_client.start_crawler(Name=S3_CUSTOMER_CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Lambda-2**: It Triggers the Glue Job when executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfn_scripts = [\"lambdas/lfn2/glue_job_triggerer.py\"]\n",
    "build_lambda_package(lfn_scripts, \"lambdas/lfn2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFN_JOB_TRIGGERER_NAME = \"glue_job_triggerer\"\n",
    "zip_file = \"./lambdas/lfn2/package.zip\"  # Change this to the actual zip file path\n",
    "\n",
    "# Create Lambda function\n",
    "with open(zip_file, 'rb') as f:\n",
    "    zipped_code = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFN_JOB_TRIGGERER_ARN = lambda_client.create_function(\n",
    "    FunctionName=LFN_JOB_TRIGGERER_NAME,\n",
    "    Runtime='python3.9',\n",
    "    Role=LFN_ROLE_ARN,\n",
    "    Handler='glue_job_triggerer.lambda_handler',\n",
    "    Code={'ZipFile': zipped_code},\n",
    "    Timeout=120,\n",
    "    Environment={\n",
    "        'Variables': {\n",
    "            'foo': 'BAR'\n",
    "        }\n",
    "    }\n",
    ")['FunctionArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lfn.update_lambda_function_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Job**: Transforme data from `raw/customers`  and load into `processed/customers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name1 = './glue_scripts/jb1_s3csv_s3parquet.py'       # The local file you want to upload\n",
    "object_name1 = \"glues_scripts/jb1_s3csv_s3parquet.py\"     # The name to save the file as in the S3 bucket\n",
    "s3.upload_file_to_s3(S3_BUCKET_GLUE_ASSETS, file_name1, object_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = 'jb1_s3csv_s3parquet'\n",
    "JOB_SCRIPT_LOCATION = f\"s3://{S3_BUCKET_GLUE_ASSETS}/{object_name1}\"\n",
    "TABLE_NAME = \"raw_customers\"\n",
    "TARGET = f\"s3://{S3_BUCKET_DATALAKE}/processed/customers/\"\n",
    "TEM_DIR = f\"s3://{S3_BUCKET_GLUE_ASSETS}/temporary/\"\n",
    "SPARK_EVENT_LOG_PATH = f\"s3://{S3_BUCKET_GLUE_ASSETS}/sparkHistoryLogs/\"\n",
    "ARGS = {\n",
    "    \"--TempDir\": f\"s3://{S3_BUCKET_GLUE_ASSETS}/temporary/\",\n",
    "    \"--library-path\": f\"s3://{S3_BUCKET_GLUE_ASSETS}/libraries\",  # Path to external libraries (JARs, compiled libraries, JDBC drivers)\n",
    "    \"--extra-py-files\": f\"s3://{S3_BUCKET_GLUE_ASSETS}/libraries/package.zip\",  # Python modules/packages (Python .zip)\n",
    "    \"--spark-event-logs-path\": SPARK_EVENT_LOG_PATH,\n",
    "    \"--job-bookmark-option\": \"job-bookmark-enable\",\n",
    "    \"--job-language\": \"python\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue.create_glue_job(JOB_NAME, JOB_SCRIPT_LOCATION, GLUE_ROLE_ARN, TEM_DIR, SPARK_EVENT_LOG_PATH)\n",
    "glue.create_glue_job_v2(JOB_NAME, JOB_SCRIPT_LOCATION, GLUE_ROLE_ARN, ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-    Package up and upload the **external libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_module = os.getenv(\"UTILS\") + \"/mylogger.py\"\n",
    "build_lambda_package(\n",
    "    [logger_module], \n",
    "    \"./external_py_lib\", \n",
    "    py_libs=['coloredlogs', 'termcolor']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_folder_to_s3(S3_BUCKET_GLUE_ASSETS,\"./external_py_lib\", \"libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parametarization of the Job [`SUCCESS`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_client.delete_object(Bucket=S3_BUCKET_GLUE_ASSETS, Key=\"libraries/package.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS.update(\n",
    "    {\n",
    "        \"--JOB_NAME\": JOB_NAME,\n",
    "        \"--S3_BUCKET_DATALAKE\": S3_BUCKET_DATALAKE,\n",
    "        \"--CATALOG_DB_NAME\": GLUE_CATALOG_DB,\n",
    "        \"--TABLE_NAME\": TABLE_NAME,\n",
    "        \"--TARGET\": TARGET,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue.start_glue_job(JOB_NAME, arguments=ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.start_job_run(JobName=JOB_NAME, Arguments=ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! aws logs tail --follow /aws-glue/jobs --filter-pattern \"SUCCEEDED\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Event Rule 1**: It matches \"Glue Crawler State Change\" pattern with target (LFN_JOB_TRIGGERER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `Event Source`: AWS Glue Crawler (S3_RAW_CRAWLER_NAME)\n",
    "-   `Event Type`: \"Glue Crawler State Change\" (crawler_rule_event_pattern)\n",
    "-   `Evnet Target`: Lambda Function (LFN_JOB_TRIGGERER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_CUSTOMER_CRAWLER_RULE_NAME = \"htech-customer-crawler-rule\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler_rule_event_pattern = {\n",
    "    \"source\": [\"aws.glue\"],\n",
    "    \"detail-type\": [\"Glue Crawler State Change\"],\n",
    "    \"detail\": {\"state\": [\"Succeeded\"], \"crawlerName\": [S3_CUSTOMER_CRAWLER_NAME]},\n",
    "}\n",
    "\n",
    "# Create EventBridge Rule to catch Glue Crawler State Change events\n",
    "S3_CUSTOMER_CRAWLER_RULE_ARN = events_client.put_rule(\n",
    "    Name=S3_CUSTOMER_CRAWLER_RULE_NAME,\n",
    "    EventPattern=json.dumps(crawler_rule_event_pattern),\n",
    "    State=\"ENABLED\",\n",
    "    Description=\"Rule to capture AWS Glue Crawler state changes\",\n",
    ")[\"RuleArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_payload = {\n",
    "    \"JOB_NAME\": JOB_NAME,\n",
    "    \"S3_BUCKET_DATALAKE\": S3_BUCKET_DATALAKE,\n",
    "    \"CATALOG_DB_NAME\": GLUE_CATALOG_DB,\n",
    "    \"TABLE_NAME\": TABLE_NAME,\n",
    "    \"TARGET\": TARGET,\n",
    "}\n",
    "\n",
    "events_client.put_targets(\n",
    "    Rule=S3_CUSTOMER_CRAWLER_RULE_NAME,\n",
    "    Targets=[\n",
    "        {\n",
    "            \"Id\": f\"{S3_CUSTOMER_CRAWLER_RULE_NAME}_job_trigger_lfn\",\n",
    "            \"Arn\": LFN_JOB_TRIGGERER_ARN,\n",
    "            \"Input\": json.dumps(input_payload),  # Convert to proper JSON string\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_payload[\"CATALOG_DB_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `Input` data is sent into Lambda function as `event`:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"JOB_NAME\": \"jb1_s3csv_s3parquet\",\n",
    "    \"S3_BUCKET_DATALAKE\": \"htech-datalake-bkt\",\n",
    "    \"CATALOG_DB_NAME\": \"htech-catalog-db\",\n",
    "    \"TABLE_NAME\": \"raw_customers\",\n",
    "    \"TARGET\": \"s3://htech-datalake-bkt/processed/customers/\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grant EventBridge permission to invoke the Lambda function\n",
    "lambda_client.add_permission(\n",
    "    FunctionName=LFN_JOB_TRIGGERER_NAME,\n",
    "    StatementId=f\"{S3_CUSTOMER_CRAWLER_RULE_NAME}-invoke-permission\",\n",
    "    Action=\"lambda:InvokeFunction\",\n",
    "    Principal=\"events.amazonaws.com\",\n",
    "    SourceArn=S3_CUSTOMER_CRAWLER_RULE_ARN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the SNS Topic as a target to the EventBridge Rule for notification purposes\n",
    "events_client.put_targets(\n",
    "    Rule=S3_CUSTOMER_CRAWLER_RULE_NAME,\n",
    "    Targets=[{\n",
    "        'Id': f\"{S3_CUSTOMER_CRAWLER_RULE_NAME}_sns_topic\",\n",
    "        'Arn': JOB_COMPLETE_TOPIC_ARN,\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **event** Data sent by 'Crawler state change event' into SNS Topic\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"version\": \"0\",\n",
    "    \"id\": \"f971dd0e-4705-d8ba-f46c-7028e8f5e0ab\",\n",
    "    \"detail-type\": \"Glue Crawler State Change\",\n",
    "    \"source\": \"aws.glue\",\n",
    "    \"account\": \"381492255899\",\n",
    "    \"time\": \"2024-10-20T15:44:53Z\",\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"resources\": [],\n",
    "    \"detail\": {\n",
    "        \"tablesCreated\": \"1\",\n",
    "        \"warningMessage\": \"N/A\",\n",
    "        \"partitionsUpdated\": \"0\",\n",
    "        \"tablesUpdated\": \"0\",\n",
    "        \"message\": \"Crawler Succeeded\",\n",
    "        \"partitionsDeleted\": \"0\",\n",
    "        \"accountId\": \"999999999999\",\n",
    "        \"runningTime (sec)\": \"26\",\n",
    "        \"tablesDeleted\": \"0\",\n",
    "        \"crawlerName\": \"htech-s3crawler\",\n",
    "        \"completionDate\": \"2024-10-20T15:44:53Z\",\n",
    "        \"state\": \"Succeeded\",\n",
    "        \"partitionsCreated\": \"0\",\n",
    "        \"cloudWatchLogLink\": \"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws-glue/crawlers;stream=httx-s3crawler\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Event Rule 2**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `Event Source`: AWS Glue Job (JOB_NAME)\n",
    "-   `Event Type`: \"Glue Job State Change\" (job_rule_event_pattern)\n",
    "-   `Evnet Target`: SNS Topic (JOB_COMPLETE_RULE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_COMPLETE_RULE_NAME = 'htech-job-complete-rule'\n",
    "job_rule_event_pattern = {\n",
    "    \"source\": [\"aws.glue\"],\n",
    "    \"detail-type\": [\"Glue Job State Change\"], # Event Type\n",
    "    \"detail\": {\n",
    "        \"jobName\": [JOB_NAME],\n",
    "        \"state\": [\"SUCCEEDED\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "response = events_client.put_rule(\n",
    "    Name=JOB_COMPLETE_RULE_NAME,\n",
    "    EventPattern=json.dumps(job_rule_event_pattern),\n",
    "    State='ENABLED',\n",
    "    Description='Rule to capture AWS Glue job state changes',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Attach the SNS Topic as a target to the EventBridge Rule for notification purposes\n",
    "events_client.put_targets(\n",
    "    Rule=JOB_COMPLETE_RULE_NAME,\n",
    "    Targets=[{\n",
    "        'Id': f\"{JOB_COMPLETE_RULE_NAME}_target\",\n",
    "        'Arn': JOB_COMPLETE_TOPIC_ARN,\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Response sent to SNS topic by \"Job Run State Change\"\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"version\": \"0\",\n",
    "    \"id\": \"ee14807c-1f89-490e-b4e6-43f071980d95\",\n",
    "    \"detail-type\": \"Glue Job State Change\",\n",
    "    \"source\": \"aws.glue\",\n",
    "    \"account\": \"999999999999\",\n",
    "    \"time\": \"2024-10-20T15:00:35Z\",\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"resources\": [],\n",
    "    \"detail\": {\n",
    "        \"jobName\": \"jb1_s3csv_s3parquet\",\n",
    "        \"severity\": \"INFO\",\n",
    "        \"state\": \"STOPPED\",\n",
    "        \"jobRunId\": \"jr_d8165d895a8eee53494238118f07659a75b1a0d192048ed8a7a429c9ce176d5c\",\n",
    "        \"message\": \"Job run stopped\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST THE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.delete_object(Bucket=S3_BUCKET_DATALAKE, Key='raw/customers/customers1.csv')\n",
    "# # s3_client.delete_object(Bucket=S3_BUCKET_DATALAKE, Key='raw/customers/customers2.csv')\n",
    "# # s3_client.delete_object(Bucket=S3_BUCKET_DATALAKE, Key='processed/customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMShah_arn = iam_client.get_user(UserName=\"AMShah\")[\"User\"][\"Arn\"]\n",
    "# lftn.grant_table_level_permissions(AMShah_arn, GLUE_CATALOG_DB, \"raw_customers\", [\"DROP\"])\n",
    "# glue_client.delete_table(DatabaseName=GLUE_CATALOG_DB, Name=\"raw_customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file('../../data/customers1.csv', S3_BUCKET_DATALAKE, 'raw/customers/customers1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **Verify Job Bookmarks are acting as expected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file('../../data/customers2.csv', S3_BUCKET_DATALAKE, 'raw/customers/customers2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws events list-rules --name-prefix {JOB_COMPLETE_RULE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pickle Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "GLUE_ROLE_NAME = \"glue-pipeline-role\"\n",
    "LFN_ROLE_NAME = \"lfn-pipeline-role\"\n",
    "GLUE_ROLE_ARN = \"arn:aws:iam::530976901147:role/glue-pipeline-role\"\n",
    "LFN_ROLE_ARN = \"arn:aws:iam::530976901147:role/lfn-pipeline-role\"\n",
    "S3_BUCKET_DATALAKE = \"htech-datalake-bkt\"\n",
    "S3_BUCKET_GLUE_ASSETS = \"htech-glue-assets-bkt\"\n",
    "DQ_OBJECT = \"Data_Quality(DQ)\"\n",
    "GLUE_CATALOG_DB = \"htech-glue-catalog-db\"\n",
    "DATALAKE_LOCATION_URI = f\"s3://{S3_BUCKET_DATALAKE}\"\n",
    "TOPIC_NAME = \"htech-sns-topic\"\n",
    "JOB_COMPLETE_TOPIC_ARN = \"arn:aws:sns:us-east-1:530976901147:htech-sns-topic\"\n",
    "LFN_CRAWLER_NAME = \"crawler_triggerer\"\n",
    "LFN_CRAWLER_ARN = \"arn:aws:lambda:us-east-1:530976901147:function:crawler_triggerer\"\n",
    "S3_CUSTOMER_CRAWLER_NAME = \"htech-s3-customer-crawler\"\n",
    "S3_CUSTOMER_CRAWLER_TARGET = {\"S3Targets\": [{\"Path\": f\"s3://{S3_BUCKET_DATALAKE}/{'raw/customers'}\"},]}\n",
    "LFN_JOB_TRIGGERER_NAME = \"glue_job_triggerer\"\n",
    "LFN_JOB_TRIGGERER_ARN = (\"arn:aws:lambda:us-east-1:530976901147:function:glue_job_triggerer\")\n",
    "TEM_DIR = f\"s3://{S3_BUCKET_GLUE_ASSETS}/temporary/\"\n",
    "SPARK_EVENT_LOG_PATH = f\"s3://{S3_BUCKET_GLUE_ASSETS}/sparkHistoryLogs/\"\n",
    "TABLE_NAME = \"raw_customers\"\n",
    "TARGET = f\"s3://{S3_BUCKET_DATALAKE}/processed/customers/\"\n",
    "JOB_NAME = \"jb1_s3csv_s3parquet\"\n",
    "file_name1 = \"./glue_scripts/jb1_s3csv_s3parquet.py\"\n",
    "object_name1 = \"glues_scripts/jb1_s3csv_s3parquet.py\"\n",
    "JOB_SCRIPT_LOCATION = f\"s3://{S3_BUCKET_GLUE_ASSETS}/{object_name1}\"\n",
    "ARGS = {\n",
    "    \"--TempDir\": f\"s3://{S3_BUCKET_GLUE_ASSETS}/temporary/\",\n",
    "    \"--library-path\": f\"s3://{S3_BUCKET_GLUE_ASSETS}/libraries\",  # Path to external libraries (JARs, compiled libraries, JDBC drivers)\n",
    "    \"--extra-py-files\": f\"s3://{S3_BUCKET_GLUE_ASSETS}/libraries/package.zip\",  # Python modules/packages (Python .zip)\n",
    "    \"--spark-event-logs-path\": SPARK_EVENT_LOG_PATH,\n",
    "}\n",
    "\n",
    "S3_CUSTOMER_CRAWLER_RULE_NAME = \"htech-raw-crawler-rule\"\n",
    "S3_CUSTOMER_CRAWLER_RULE_ARN = (\"arn:aws:events:us-east-1:530976901147:rule/htech-raw-crawler-rule\")\n",
    "JOB_COMPLETE_RULE_NAME = \"htech-job-complete-rule\"\n",
    "JOB_COMPLETE_RULE_ARN = (\"arn:aws:events:us-east-1:530976901147:rule/htech-job-complete-rule\")\n",
    "S3_SALES_CRAWLER = \"htech-s3-sales-crawler\"\n",
    "S3_SALES_CRAWLER_TARGET = {\"S3Targets\": [{\"Path\": f\"s3://{S3_BUCKET_DATALAKE}/raw/sales\"}]}\n",
    "\n",
    "# Save to file\n",
    "with open(\"event_based_etl_ipynb.pkl\", \"wb\") as f:\n",
    "    pickle.dump((GLUE_ROLE_NAME, LFN_ROLE_NAME, GLUE_ROLE_ARN, LFN_ROLE_ARN, S3_BUCKET_DATALAKE, S3_BUCKET_GLUE_ASSETS, GLUE_CATALOG_DB, DATALAKE_LOCATION_URI, TOPIC_NAME, JOB_COMPLETE_TOPIC_ARN, LFN_CRAWLER_NAME, LFN_CRAWLER_ARN, S3_CUSTOMER_CRAWLER_NAME, S3_CUSTOMER_CRAWLER_TARGET, S3_SALES_CRAWLER, S3_SALES_CRAWLER_TARGET, LFN_JOB_TRIGGERER_NAME, LFN_JOB_TRIGGERER_ARN, TEM_DIR, SPARK_EVENT_LOG_PATH, TABLE_NAME, TARGET, JOB_NAME, JOB_SCRIPT_LOCATION, ARGS, S3_CUSTOMER_CRAWLER_RULE_NAME, S3_CUSTOMER_CRAWLER_RULE_ARN, JOB_COMPLETE_RULE_NAME, JOB_COMPLETE_RULE_ARN), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Crawler 2**: NOT TESTED !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catalog Data from `processed/customer` as a table by the name `processed_customers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_PROCESSED_CRAWLER_NAME = \"httx-s3-processed-crawler\"\n",
    "S3_PROCESSED_CRAWLER_TARGET = {\n",
    "    'S3Targets': [{'Path': f\"s3://{S3_BUCKET_DATALAKE}/{'processed/customers'}\"},]\n",
    "}\n",
    "glue.create_glue_crawler(S3_PROCESSED_CRAWLER_NAME, GLUE_ROLE_ARN, GLUE_CATALOG_DB, S3_PROCESSED_CRAWLER_TARGET, table_prefix='processed_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue_client.start_crawler(Name=S3_CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMominNJ_arn = iam_client.get_user(UserName='AMominNJ')['User']['Arn']\n",
    "lftn.grant_table_level_permissions(AMominNJ_arn, GLUE_CATALOG_DB, 'processed_customers', ['DROP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.delete_database(CatalogId=ACCOUNT_ID,Name=GLUE_CATALOG_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "bucket1 = s3.Bucket(S3_BUCKET_DATALAKE)\n",
    "bucket2 = s3.Bucket(S3_BUCKET_GLUE_ASSETS)\n",
    "\n",
    "# Delete all objects in the bucket\n",
    "bucket1.objects.all().delete()\n",
    "bucket2.objects.all().delete()\n",
    "\n",
    "# Delete all object versions (if versioning is enabled)\n",
    "# bucket1.object_versions.all().delete()\n",
    "# bucket2.object_versions.all().delete()\n",
    "\n",
    "# Finally, delete the bucket\n",
    "bucket1.delete()\n",
    "bucket2.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.delete_crawler(Name=S3_CUSTOMER_CRAWLER_NAME)\n",
    "# glue_client.delete_crawler(Name=S3_PROCESSED_CRAWLER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client.delete_job(JobName=JOB_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.delete_sns_topic(JOB_COMPLETE_TOPIC_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = events_client.list_targets_by_rule(Rule=S3_CUSTOMER_CRAWLER_RULE_NAME)['Targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_client.remove_targets(Rule=S3_CUSTOMER_CRAWLER_RULE_NAME, Ids=[targets[0]['Id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_client.delete_rule(Name=S3_CUSTOMER_CRAWLER_RULE_NAME,Force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = events_client.list_targets_by_rule(Rule=JOB_COMPLETE_RULE_NAME)['Targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_client.remove_targets(Rule=JOB_COMPLETE_RULE_NAME, Ids=[targets[0]['Id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_client.delete_rule(Name=JOB_COMPLETE_RULE_NAME,Force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = lambda_client.remove_permission(\n",
    "#     FunctionName=LFN_CRAWLER_NAME,\n",
    "#     StatementId='s3-invoke-permission'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE IAM ROLE AT THE END AFTER DELETING ALL OTHER RESOURCES.\n",
    "iam.delete_iam_role(GLUE_ROLE_NAME)\n",
    "iam.delete_iam_role(LFN_ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_client.delete_function(FunctionName=LFN_CRAWLER_NAME)\n",
    "lambda_client.delete_function(FunctionName=LFN_JOB_TRIGGERER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgroups = [\n",
    "    \"/aws-glue/crawlers\",\n",
    "    \"/aws-glue/jobs/output\",\n",
    "    \"/aws-glue/jobs/error\",\n",
    "    \"/aws-glue/jobs/logs-v2\",\n",
    "    \"/aws/lambda/crawler_triggerer\",\n",
    "    \"/aws/lambda/glue_job_triggerer\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the log group:\n",
    "[logs_client.delete_log_group(logGroupName=lg) for lg in lgroups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [KA: Complete Master Class on Pydeequ & AWS Glue Data Quality for ETL Pipelines](https://www.youtube.com/watch?v=699jUhUE9hY&list=PLO95rE9ahzRsdzmZ_ZT-3uOn1Nh2eEpWB&index=29&t=4151s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Doc: PyDeequ](https://pydeequ.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   os.environ[\"PYSPARK_PYTHON\"] = \"3.1\"\n",
    "-   pydeequ==\"1.1.0\"\n",
    "-   Glue Version: Glue 3.0 - Supports spark 3.1, Scala 2, Python 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Jar Download: https://mvnrepository.com/artifact/com.amazon.deequ/deequ/2.0.0-spark-3.1\n",
    "-   PyDeeqe Download: https://pypi.org/project/pydeequ/\n",
    "    -   Used Python Module: pydeequ==1.1.0\n",
    "job_parameters={\n",
    "    '--aditional-python-modules': 'pydeequ==1.1.0'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_parameters={\n",
    "    \"--aditional-python-modules\": \"pydeequ==1.1.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```txt\n",
    "ImportError: Unable to import required dependencies:\n",
    "numpy: Error importing numpy: you should not try to import numpy from\n",
    "        its source directory; please exit the numpy source tree, and relaunch\n",
    "        your python interpreter from there.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.put_object(Bucket=S3_BUCKET_DATALAKE, Key=\"raw/iris\")\n",
    "s3_client.put_object(Bucket=S3_BUCKET_DATALAKE, Key=\"Data-Quality(DQ)/iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_client.upload_file('../../data/iris.csv', S3_BUCKET_DATALAKE, 'raw/iris/iris.csv')\n",
    "s3_client.upload_file(\n",
    "    \"./external_java/deequ-2.0.7-spark-3.5.jar\",\n",
    "    S3_BUCKET_GLUE_ASSETS,\n",
    "    \"libraries/deequ-2.0.7-spark-3.5.jar\",\n",
    ")\n",
    "# s3_client.delete_object(Bucket=S3_BUCKET_DATALAKE, Key=\"raw/iris/iris.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **Iris Job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name1 = './glue_scripts/iris_job_dq_val.py'       # The local file you want to upload\n",
    "object_name1 = \"glues_scripts/iris_job_dq_val.py\"     # The name to save the file as in the S3 bucket\n",
    "s3.upload_file_to_s3(S3_BUCKET_GLUE_ASSETS, file_name1, object_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_JOB_NAME = 'iris_dq_val_job'\n",
    "SOURCE_DATA = f\"s3://{S3_BUCKET_DATALAKE}/raw/iris/iris.csv\"\n",
    "DQ_VALIDATION_OUTPUT = f\"s3://{S3_BUCKET_DATALAKE}/Data-Quality(DQ)/iris/\"\n",
    "\n",
    "IRIS_JOB_SCRIPT_LOCATION = f\"s3://{S3_BUCKET_GLUE_ASSETS}/{object_name1}\"\n",
    "TEM_DIR = f\"s3://{S3_BUCKET_GLUE_ASSETS}/temporary/\"\n",
    "SPARK_EVENT_LOG_PATH = f\"s3://{S3_BUCKET_GLUE_ASSETS}/sparkHistoryLogs/\"\n",
    "DEFAULT_ARGS = {\n",
    "    \"--TempDir\": f\"s3://{S3_BUCKET_GLUE_ASSETS}/temporary/\",\n",
    "    \"--library-path\": f\"s3://{S3_BUCKET_GLUE_ASSETS}/libraries/deequ-2.0.7-spark-3.5.jar\",  # Path to external libraries (JARs, compiled libraries, JDBC drivers)\n",
    "    \"--aditional-python-modules\": \"pydeequ==1.5.0\",\n",
    "    \"--extra-py-files\": f\"s3://{S3_BUCKET_GLUE_ASSETS}/libraries/package.zip\",  # Python modules/packages (Python .zip)\n",
    "    \"--spark-event-logs-path\": SPARK_EVENT_LOG_PATH,\n",
    "    # \"--job-bookmark-option\": \"job-bookmark-enable\",\n",
    "    \"--job-language\": \"python\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue.create_glue_job_v2(IRIS_JOB_NAME, IRIS_JOB_SCRIPT_LOCATION, GLUE_ROLE_ARN, DEFAULT_ARGS, glue_version='5.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-    Package up and upload the **external libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_module = os.getenv(\"UTILS\") + \"/mylogger.py\"\n",
    "build_lambda_package(\n",
    "    [logger_module],\n",
    "    \"./external_py_lib\",\n",
    "    py_libs=[\n",
    "        \"coloredlogs\",\n",
    "        \"termcolor\",\n",
    "        \"pydeequ==1.5.0\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_client.delete_object(Bucket=S3_BUCKET_GLUE_ASSETS, Key=\"libraries/package.zip\")\n",
    "s3.upload_folder_to_s3(S3_BUCKET_GLUE_ASSETS,\"./external_py_lib\", \"libraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_ARGUMENTS = {\n",
    "    \"--IRIS_JOB_NAME\": IRIS_JOB_NAME,\n",
    "    \"--SOURCE_DATA\": SOURCE_DATA,\n",
    "    \"--DQ_VALIDATION_OUTPUT\": DQ_VALIDATION_OUTPUT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_JOB_RUN_ID = glue_client.start_job_run(JobName=IRIS_JOB_NAME, Arguments=JOB_ARGUMENTS)[\"JobRunId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_JOB_RUN_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
